{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pCoYriKf1EJZ"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers==4.51.3\n",
    "!pip install -q kaggle==1.7.4.2\n",
    "!pip install -q dill==0.3.8\n",
    "!pip install -q datasets==3.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQ7BCC4LOZJW"
   },
   "source": [
    "### Let's import packages for data processing, BERT model, and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3MNcv7qS1J5M"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizerFast, Trainer, TrainingArguments\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44kK3-50OsYH"
   },
   "source": [
    "### Use Kaggle API token to download datasets for training\n",
    "\n",
    "You can create an account in Kaggle and download the file kaggle.json into your local computer. Then you can upload it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "a-L8IIlo1N7j",
    "outputId": "83d16b40-0d1e-41ef-dd4c-88ddf730100e"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()\n",
    "!mkdir /.kaggle\n",
    "!mv kaggle.json /.kaggle\n",
    "!mv /.kaggle /root/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_RThYutPqBj"
   },
   "source": [
    "### Let's download datasets\n",
    "For training our BERT model we will use 3 sources:\n",
    "\n",
    "\n",
    "*   Lingspam dataset\n",
    "*   Spamassassin dataset\n",
    "*   Sms-spam-collection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EbLyXmqd1QqV",
    "outputId": "59975d54-d785-4caf-a6a0-abca7c51d6e8"
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download -d mandygu/lingspam-dataset\n",
    "!kaggle datasets download -d uciml/sms-spam-collection-dataset\n",
    "!kaggle datasets download -d bertvankeulen/spamassassin-spam\n",
    "!unzip lingspam-dataset.zip\n",
    "!unzip spamassassin-spam.zip\n",
    "!unzip sms-spam-collection-dataset.zip\n",
    "!rm -rf lingspam-dataset.zip\n",
    "!rm -rf sms-spam-collection-dataset.zip\n",
    "!rm -rf spamassassin-spam.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYrlyyp5QJlQ"
   },
   "source": [
    "### Functions for data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mpEZuRrcHyOT"
   },
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_path_or_name: str):\n",
    "    \"\"\"\n",
    "    Load a BERT model and tokenizer for sequence classification.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_path_or_name : str\n",
    "        Path to a local model directory or the name of a pretrained model on Hugging Face hub.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : BertForSequenceClassification\n",
    "        The loaded BERT model for sequence classification.\n",
    "\n",
    "    tokenizer : BertTokenizerFast\n",
    "        The tokenizer associated with the model.\n",
    "    \"\"\"\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path_or_name)\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(model_path_or_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "def process_data(data, tokenizer, column='message', padding=True,\n",
    "                 truncation=True):\n",
    "    \"\"\"\n",
    "    Tokenize input text data using the specified tokenizer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dict\n",
    "        Dictionary containing a key corresponding to `column` with text data.\n",
    "\n",
    "    tokenizer : PreTrainedTokenizerFast\n",
    "        Tokenizer to apply.\n",
    "\n",
    "    column : str, optional\n",
    "        Name of the text column to tokenize (default is 'message').\n",
    "\n",
    "    padding : bool, optional\n",
    "        Whether to pad sequences to the same length (default is True).\n",
    "\n",
    "    truncation : bool, optional\n",
    "        Whether to truncate sequences that are too long (default is True).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Tokenized output suitable for model input.\n",
    "    \"\"\"\n",
    "    return tokenizer(data[column], padding=padding, truncation=truncation)\n",
    "\n",
    "def removeUrlAndHtml(df, column='message'):\n",
    "    \"\"\"\n",
    "    Remove URLs and HTML tags from a text column in a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input DataFrame.\n",
    "\n",
    "    column : str, optional\n",
    "        Name of the column containing text to clean (default is 'message').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with cleaned text.\n",
    "    \"\"\"\n",
    "    regexes = [\n",
    "        r\"https?://\\S+|www\\.\\S+\", r\"<[^>]>\"\n",
    "    ]\n",
    "\n",
    "    for reg in regexes:\n",
    "        df[column] = df[column].str.replace(reg, \"\", regex=True)\n",
    "    return df\n",
    "\n",
    "def preprocess_data(file_mapping, test_size=0.3):\n",
    "    \"\"\"\n",
    "    Load, clean, and split multiple CSV datasets into train and test datasets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_mapping : dict\n",
    "        Mapping of file paths to configuration dictionaries. Each configuration may contain:\n",
    "            - 'encoding': File encoding.\n",
    "            - 'columns': Mapping of original to new column names.\n",
    "            - 'map': Optional. Mapping of values in specific columns.\n",
    "            - 'regex': Optional. Regex replacements per column.\n",
    "\n",
    "    test_size : float, optional\n",
    "        Proportion of data to reserve for testing (default is 0.3).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_dataset : Dataset\n",
    "        The training subset of the dataset.\n",
    "\n",
    "    test_dataset : Dataset\n",
    "        The testing subset of the dataset.\n",
    "    \"\"\"\n",
    "    df_concat = pd.DataFrame(columns=['message','label'])\n",
    "    for file, conf in file_mapping.items():\n",
    "        df = pd.read_csv(file, encoding=conf['encoding'], on_bad_lines='skip',\n",
    "                          engine='python')\n",
    "        df.rename(columns=conf['columns'], inplace=True)\n",
    "\n",
    "        if 'map' in conf:\n",
    "            for column, value_mapping in conf['map'].items():\n",
    "                df[column] = df[column].map(value_mapping)\n",
    "        if 'regex' in conf:\n",
    "            for column, regex_exp in conf['regex'].items():\n",
    "                df[column] = df[column].str.replace(regex_exp, \"\", regex=True)\n",
    "\n",
    "        print(f\"Dataset {file} has {len(df)} records.\")\n",
    "        df_concat = pd.concat([df_concat, df[list(conf['columns'].values())]])\n",
    "\n",
    "    print(f\"Total amount of records in consolidated dataset is: \\\n",
    "        {len(df_concat)}\")\n",
    "\n",
    "    # Let's remove urls and html tags\n",
    "    df_concat = removeUrlAndHtml(df_concat)\n",
    "    print(f\"Total amount of records in consolidated dataset is after cleaning \\\n",
    "    urls and html tags: {len(df_concat)}\")\n",
    "\n",
    "    dataset = Dataset.from_pandas(df_concat)\n",
    "    dataset = dataset.train_test_split(test_size=test_size)\n",
    "    return dataset['train'], dataset['test']\n",
    "\n",
    "def prepare_dataset(dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    Apply tokenization and formatting to a Hugging Face dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : Dataset\n",
    "        Hugging Face dataset to be tokenized.\n",
    "\n",
    "    tokenizer : PreTrainedTokenizerFast\n",
    "        Tokenizer to apply.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dataset\n",
    "        Tokenized dataset formatted for PyTorch.\n",
    "    \"\"\"\n",
    "    dataset = dataset.map(lambda x: process_data(x, tokenizer), batched=True,\n",
    "                          batch_size=len(dataset))\n",
    "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask',\n",
    "                                              'label'])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_P1AUTesQhhH"
   },
   "source": [
    "### Functions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wf0zfqBXQmSi"
   },
   "outputs": [],
   "source": [
    "def train(model, train_dataset, output_dir='./results', log_dir='./logs',\n",
    "          epochs=10, batch_size=16):\n",
    "    \"\"\"\n",
    "    Fine-tune a Hugging Face model using the Trainer API.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : PreTrainedModel\n",
    "        The Hugging Face model to be trained.\n",
    "\n",
    "    train_dataset : Dataset\n",
    "        The training dataset (Hugging Face `datasets.Dataset` object).\n",
    "\n",
    "    output_dir : str, optional\n",
    "        Directory to save model checkpoints and final model (default is './results').\n",
    "\n",
    "    log_dir : str, optional\n",
    "        Directory to store training logs for TensorBoard or other logging tools (default is './logs').\n",
    "\n",
    "    epochs : int, optional\n",
    "        Number of training epochs (default is 10).\n",
    "\n",
    "    batch_size : int, optional\n",
    "        Batch size per device during training (default is 16).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Trainer\n",
    "        The Hugging Face `Trainer` object after training is completed.\n",
    "    \"\"\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"no\",\n",
    "        logging_dir=log_dir,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsGjYKt9Qs1l"
   },
   "source": [
    "### Functions for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-16Tcr0QwMR"
   },
   "outputs": [],
   "source": [
    "def evaluate(trainer, test_dataset):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model on a test dataset and display classification metrics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trainer : Trainer\n",
    "        Hugging Face Trainer object that has been trained.\n",
    "\n",
    "    test_dataset : Dataset\n",
    "        The dataset to evaluate on (must contain a 'label' column).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Prints the classification report and displays the confusion matrix plot.\n",
    "    \"\"\"\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    predicted_labels = predictions.predictions.argmax(axis=-1)\n",
    "    true_labels = test_dataset['label'].numpy()\n",
    "\n",
    "    print(classification_report(true_labels, predicted_labels))\n",
    "\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"HAM\", \"SPAM\"])\n",
    "    disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbV1L8IjQ4_l"
   },
   "source": [
    "### Main Workflow\n",
    "\n",
    "This function orchestrates the full text classification pipeline using BERT.\n",
    "\n",
    "It performs the following steps based on the `train_new_model` flag:\n",
    "\n",
    "1. **Load and preprocess data** from multiple CSV sources using a configuration dictionary (`file_mapping`) that specifies column mappings, encodings, optional label mappings, and regex cleaning rules.\n",
    "\n",
    "2. If `train_new_model=True`:\n",
    " - Loads a pretrained BERT model and tokenizer (default: 'bert-base-uncased').\n",
    " - Preprocesses and tokenizes the datasets.\n",
    " - Trains the model using Hugging Face's `Trainer`.\n",
    " - Saves the trained model and tokenizer to the `./saved_model/` directory.\n",
    " - Compresses the model directory into a `.zip` file and downloads it.\n",
    "\n",
    "3. If `train_new_model=False`:\n",
    " - Loads a previously saved model and tokenizer from `./saved_model/`.\n",
    " - Prepares the test set for evaluation only.\n",
    "\n",
    "4. **Evaluation**:\n",
    " - Runs the model on the test dataset.\n",
    " - Prints a classification report (precision, recall, F1-score).\n",
    " - Displays a confusion matrix plot for HAM vs SPAM classification.\n",
    "\n",
    "Usage:\n",
    "-------\n",
    "- To train a new model from scratch and save it:\n",
    " main(train_new_model=True)\n",
    "\n",
    "- To skip training and only evaluate a saved model:\n",
    " main(train_new_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 724,
     "referenced_widgets": [
      "6d57e26ce5f34360a6106862a8195d3c",
      "183f36986e1a445e8dcda3b6d298a59d",
      "2d518f42197740268f1c1eb46229cdf8",
      "bd8b71fc5df043feb3ea63c42959c822",
      "15be6692cf5f45b3a0c12e1e1cd19f80",
      "3bf3433908f749d19342e92443dfccd8",
      "043f59fa6ef04d6a8c272f3ee3c22f00",
      "f573d8015e224665bede324d05a1bf62",
      "d7832dcc6dd44cf2a93ec9daff4dd5ab",
      "f258c47776d64a88bcb2f522342c5de2",
      "3c749f1e666a48c284ef4f5f645badd6"
     ]
    },
    "id": "gjZtpSFOQ3H6",
    "outputId": "7611cc27-92ee-45d0-dd0e-8898292605a1"
   },
   "outputs": [],
   "source": [
    "def main(train_new_model=True):\n",
    "    file_mapping = {\n",
    "        './messages.csv':\n",
    "        {\n",
    "            'columns': {\n",
    "                'label': 'label',\n",
    "                'message': 'message',\n",
    "            },\n",
    "            'encoding': 'utf-8',\n",
    "        },\n",
    "        './spam.csv':\n",
    "        {\n",
    "            'columns': {\n",
    "                'v1': 'label',\n",
    "                'v2': 'message',\n",
    "            },\n",
    "            'encoding': 'latin-1',\n",
    "            'map': {\n",
    "                'label': {\n",
    "                    'ham': 0,\n",
    "                    'spam': 1,\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        './SA_SubTxt_fn.csv':\n",
    "        {\n",
    "            'columns': {\n",
    "                'label': 'label',\n",
    "                'data': 'message',\n",
    "            },\n",
    "            'encoding': 'utf-8',\n",
    "            'regex': {\n",
    "                'message': r\"^\\[|\\]$\"\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if train_new_model:\n",
    "\n",
    "        test_size = 0.3\n",
    "        model_path = 'bert-base-uncased'  # Pretrained model base\n",
    "        model, tokenizer = load_model_and_tokenizer(model_path)\n",
    "\n",
    "        train_set, test_set = preprocess_data(file_mapping, test_size)\n",
    "\n",
    "        train_set = prepare_dataset(train_set, tokenizer)\n",
    "        test_set = prepare_dataset(test_set, tokenizer)\n",
    "\n",
    "        trainer = train(model, train_set)\n",
    "\n",
    "        # Save model and tokenizer\n",
    "        trainer.save_model('./saved_model')\n",
    "        tokenizer.save_pretrained('./saved_model')\n",
    "\n",
    "        # Zip the result directory\n",
    "        shutil.make_archive('saved_model', 'zip', './saved_model')\n",
    "        files.download('saved_model.zip')\n",
    "\n",
    "    else:\n",
    "        # Load already saved model\n",
    "        model, tokenizer = load_model_and_tokenizer('./saved_model')\n",
    "\n",
    "        _, test_set = preprocess_data(file_mapping)\n",
    "        test_set = prepare_dataset(test_set, tokenizer)\n",
    "\n",
    "        trainer = Trainer(model=model)  # No need for training arguments when just evaluating\n",
    "\n",
    "    # Evaluate in both cases\n",
    "    evaluate(trainer, test_set)\n",
    "main(False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
